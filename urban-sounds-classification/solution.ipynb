{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from time import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import NMF\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# road to file : should be changed if not executed on the original machine\n",
    "FileRoot = '/home/ahmed/Documents/'\n",
    "\n",
    "# Load train, validation, test as DafaFrames:\n",
    "files = pd.read_table(os.path.join(FileRoot,'audio/train.txt'), sep='\\s+', names=['file','label'])\n",
    "validation = pd.read_table(os.path.join(FileRoot,'audio/dev.txt'), sep='\\s+', names=['file','label'])\n",
    "test = pd.read_table('test_files.txt', sep='\\s+', names=['file'])\n",
    "\n",
    "# labels of each audio file in training set and validation set\n",
    "train_class = files['label'].as_matrix()\n",
    "classes, train_labels = np.unique(train_class, return_inverse=True)\n",
    "\n",
    "validation_class = validation['label'].as_matrix()\n",
    "classes, validation_labels = np.unique(validation_class, return_inverse=True)\n",
    "\n",
    "# concatenation of training and validation files to have new training set\n",
    "training = pd.concat([files, validation], ignore_index=True)\n",
    "allTrain = training['label'].as_matrix()\n",
    "# extraction of labels of the new concatenated dataset\n",
    "classes, allTrain = np.unique(allTrain, return_inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note: </b> We have combined train files and test files to obtain ne training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of Late Fusion For train and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Functions defined below do the combined and Late Fusion: We should specify the value of the \"k\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combinedFusion(df, labels, n_mfcc, k=1):\n",
    "    \"\"\"\n",
    "    This function do both combined fusion and late fusion.\n",
    "    This function is used with training and validation set.\n",
    "    Late fusion is done if k=1\n",
    "    this function returns X_combined, y (vector of labels) and the expansion factor of each observation\n",
    "    \"\"\"\n",
    "    for i, afile in df.iterrows():\n",
    "        y_,sr = librosa.load(os.path.join(FileRoot, afile.file), sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=y_, n_fft=4096, hop_length=4096, n_mfcc=n_mfcc)\n",
    "       \n",
    "        if i==0:\n",
    "            X = mfcc.T\n",
    "            rows = X.shape[0]/k\n",
    "            X = X[0:k*rows].reshape(rows,k,n_mfcc)\n",
    "            X = np.mean(X,axis=1)\n",
    "            y = labels[i]*np.ones(X.shape[0])\n",
    "        else:\n",
    "            tmp = mfcc.T\n",
    "            tmp = tmp[0:k*(tmp.shape[0]/k)].reshape(tmp.shape[0]/k,k,n_mfcc)\n",
    "            tmp = np.mean(tmp,axis=1)\n",
    "            X = np.concatenate((X, tmp), axis=0)\n",
    "            y = np.concatenate((y, labels[i]*np.ones(tmp.shape[0])), axis=0)\n",
    "    y = y.astype(int)\n",
    "    print \"X combined fusion: \",X.shape\n",
    "    print \"y combined fusion: \",y.shape\n",
    "    return X,y,rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combinedFusion_test(df, n_mfcc, k=1):\n",
    "    \"\"\"\n",
    "    This function do both comnined or late fusion for test set.\n",
    "    df: test dataframe\n",
    "    n_mfcc: number of mfcc coeffecients\n",
    "    k: number of lines used to calculate the mean\n",
    "    if k=1: Late fusion is done\n",
    "    it returns X_test after doing this operation and the expansion factor of each line of X_test.\n",
    "    \"\"\"\n",
    "    for i, afile in df.iterrows():\n",
    "        y_,sr = librosa.load(os.path.join(FileRoot, afile.file), sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=y_, n_fft=4096, hop_length=4096, n_mfcc=n_mfcc)\n",
    "        if i==0:\n",
    "            X = mfcc.T\n",
    "            rows = X.shape[0]/k\n",
    "            X = X[0:k*rows].reshape(rows,k,n_mfcc)\n",
    "            X = np.mean(X,axis=1)\n",
    "        else:\n",
    "            tmp = mfcc.T\n",
    "            tmp = tmp[0:k*(tmp.shape[0]/k)].reshape(tmp.shape[0]/k,k,n_mfcc)\n",
    "            tmp = np.mean(tmp,axis=1)\n",
    "            X = np.concatenate((X, tmp), axis=0)\n",
    "    print \"X combined fusion: \",X.shape\n",
    "    return X,rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Calculation: (Used For Validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pred_score(y_predict, y_labels):\n",
    "    \"\"\"\n",
    "    comparing the predicted result with the true labels.\n",
    "    The function returns the accuracy.\n",
    "    \"\"\"\n",
    "    res = y_predict - y_labels\n",
    "    score = 100*(y_labels.shape[0]-np.count_nonzero(res))/float(y_labels.shape[0])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Tranformations on Train and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformation on initial Train and Validation Datasets (this tranformation won't be used later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X combined fusion:  (68676, 40)\n",
      "y combined fusion:  (68676,)\n",
      "X combined fusion:  (34220, 40)\n",
      "y combined fusion:  (34220,)\n",
      "done in 69.496s\n"
     ]
    }
   ],
   "source": [
    "# For local training and validation:\n",
    "t0 = time()\n",
    "n_mfcc = 40\n",
    "X_train,y_train,rows_train = combinedFusion(files, train_labels, n_mfcc=n_mfcc, k=1)\n",
    "X_val,y_val,rows_val = combinedFusion(validation, validation_labels, n_mfcc=n_mfcc, k=1)\n",
    "\n",
    "fusion_time = (time() - t0)\n",
    "print \"done in %0.3fs\" % fusion_time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformation on Final Train Dataset (intial Train Dataset combined with validation) and Test Dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X combined fusion:  (102896, 40)\n",
      "y combined fusion:  (102896,)\n",
      "X combined fusion:  (35164, 40)\n",
      "done in 97.796s\n"
     ]
    }
   ],
   "source": [
    "# For the final test: take both training set and validation set as the training data\n",
    "t0 = time()\n",
    "n_mfcc = 40 #number of mfcc componnets\n",
    "X,y,rows = combinedFusion(training, allTrain, n_mfcc=n_mfcc, k=1) # X : train data of combined intial train and validation\n",
    "X_test, rows_test = combinedFusion_test(test, n_mfcc=n_mfcc, k=1) \n",
    "fusion_time = (time() - t0)\n",
    "print \"done in %0.3fs\" % fusion_time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Vectors after applying Late Fusion into CSV Files (to be loaded another time without doing calculation again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save initial training and validation vectors after Late fusion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for training and validation sets:\n",
    "np.savetxt(\"X_training.csv\", X_train, delimiter=',')\n",
    "np.savetxt(\"y_training.txt\", y_train, fmt='%d')\n",
    "np.savetxt(\"X_validation.csv\", X_val, delimiter=',')\n",
    "np.savetxt(\"y_validation.txt\", y_val, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Final training and test vectors after Late fusion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used For Submission set:\n",
    "np.savetxt(\"X_train_mixed.csv\", X, delimiter=',')\n",
    "np.savetxt(\"y_train_mixed.txt\", y, fmt='%d')\n",
    "np.savetxt(\"X_test.csv\", X_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Vectors :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for training and validation sets:\n",
    "X_train = pd.read_csv('X_training.csv', sep=',', header=None).values\n",
    "X_val = pd.read_csv('X_validation.csv', sep=',', header=None).values\n",
    "y_train = np.loadtxt('y_training.txt', dtype=np.int)\n",
    "y_val = np.loadtxt('y_validation.txt', dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used For submission:\n",
    "X_train = pd.read_csv('X_train_mixed.csv', sep=',', header=None).values\n",
    "y_train = np.loadtxt('y_train_mixed.txt', dtype=np.int)\n",
    "X_test = pd.read_csv('X_test.csv', sep=',', header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MajorityVote(res):\n",
    "    \"\"\"\n",
    "    This Function computes prdicted label using majority rule.\n",
    "    returns final vector of predictions\n",
    "    \"\"\"\n",
    "    counts = np.bincount(res)\n",
    "    result = np.argmax(counts)\n",
    "    return result\n",
    "\n",
    "def VoteFusion(y_fusioned, df, rows):\n",
    "    \"\"\"\n",
    "    Before computing the vote Result after fusion, we do a reshape \n",
    "    operation to get exactly the same rows number as test files. \n",
    "    (i.e : number of lines = 290, n_columns: len(y_pred)/290)\n",
    "    Then over each row, apply the function MajorityVote to get a majority vote result\n",
    "    the function returns the final vector of predicted labels (290,)\n",
    "    \"\"\"\n",
    "    y = y_fusioned.reshape((df.shape[0], rows))\n",
    "    return np.apply_along_axis(MajorityVote, axis=1, arr=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_mfcc, whiten=True, svd_solver='auto')\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search For MLP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of code take so much time to be finished (brute search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"param_grid = [\\n  {'hidden_layer_sizes': [50,100,150,200], 'alpha': np.logspace(-4,2,7), 'tol': np.logspace(-4,-1,4)}\\n ]\\nmlp = MLPClassifier()\\nclf = GridSearchCV(mlp, param_grid)\\nclf.fit(X_train, y_train)\\nclf.best_params_\\n\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"param_grid = [\n",
    "  {'hidden_layer_sizes': [50,100,150,200], 'alpha': np.logspace(-4,2,7), 'tol': np.logspace(-4,-1,4)}\n",
    " ]\n",
    "mlp = MLPClassifier()\n",
    "clf = GridSearchCV(mlp, param_grid)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 59.034s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(300), tol=0.001,alpha=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = (time() - t0)\n",
    "print \"done in %0.3fs\" % train_time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35164,)\n"
     ]
    }
   ],
   "source": [
    "# For submission:\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = y_pred.astype(int)\n",
    "print y_pred.shape\n",
    "res = VoteFusion(y_pred, test, rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('y_pred.txt', res, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
